{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa8ad6ec-6ff7-4a8b-9d47-e35dd30f4037",
   "metadata": {},
   "source": [
    "Pandas (.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d03c41f-e25b-49a9-b2e5-21ce8da6d092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                       texto_review\n",
      "0   1             O produto é excelente e chegou rápido!\n",
      "1   2  Não gostei, a caixa veio amassada e o produto ...\n",
      "2   3       Comprei para minha mãe, ela adorou as cores.\n",
      "3   4            Péssimo atendimento, nunca mais compro.\n",
      "4   5         O celular é bom, mas a bateria dura pouco.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# Simulando um arquivo CSV (normalmente você leria com pd.read_csv('arquivo.csv'))\n",
    "csv_data = \"\"\"id;texto_review\n",
    "1;O produto é excelente e chegou rápido!\n",
    "2;Não gostei, a caixa veio amassada e o produto sujo.\n",
    "3;Comprei para minha mãe, ela adorou as cores.\n",
    "4;Péssimo atendimento, nunca mais compro.\n",
    "5;O celular é bom, mas a bateria dura pouco.\n",
    "\"\"\"\n",
    "\n",
    "# Carregando para o Pandas\n",
    "# O separador ';' é comum em arquivos brasileiros\n",
    "df = pd.read_csv(io.StringIO(csv_data), sep=';')\n",
    "\n",
    "# Vamos ver como o Pandas enxerga isso\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f0a22-b579-4c5a-9040-caaab30c303c",
   "metadata": {},
   "source": [
    "pd:\n",
    "\n",
    "    Vem da linha: import pandas as pd.\n",
    "\n",
    "    O que é: É um apelido (alias). Poderíamos ter chamado de qualquer coisa (como import pandas as abacaxi), mas a comunidade mundial de Python combinou de usar pd para não ter que digitar a palavra \"pandas\" inteira toda vez que chamar uma função.\n",
    "\n",
    "df:\n",
    "\n",
    "    Vem de DataFrame.\n",
    "\n",
    "    O que é: É o nome da estrutura de dados principal do Pandas (aquela tabela com linhas e colunas).\n",
    "\n",
    "    Como é muito comum trabalhar com DataFrames, os programadores abreviam a variável para df para ser rápido de escrever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7a1f26-9e17-48a1-8d09-660f3b820bd7",
   "metadata": {},
   "source": [
    "Pergunta guia: Ao rodar esse código, você consegue identificar qual coluna representa o nosso Documento (unidade básica de texto) segundo a definição do seu PDF? Me diga o nome dela para avançarmos para a parte do NLTK/SpaCy\n",
    "\n",
    "A coluna texto_review é onde estão nossos dados brutos. Para o Pandas, é apenas uma string (texto), mas para nós é a fonte de informação."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae77c2d-26c1-414e-b098-1a18adaeff86",
   "metadata": {},
   "source": [
    "NLTK -> Tokenização / Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c45fee4a-0279-4585-b585-e67dad20af69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'celular', 'é', 'bom', ',', 'mas', 'a', 'bateria', 'dura', 'pouco', '.']\n",
      "Quantidade: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/matheus-\n",
      "[nltk_data]     sales/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Baixando o recurso necessário para o NLTK saber o que é pontuação\n",
    "nltk.download('punkt_tab') \n",
    "\n",
    "frase = \"O celular é bom, mas a bateria dura pouco.\"\n",
    "\n",
    "# Tokenizando (note que especificamos o idioma)\n",
    "tokens = word_tokenize(frase, language='portuguese')\n",
    "\n",
    "print(tokens)\n",
    "print(f\"Quantidade: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba73aa07-9b22-4b19-a700-109c5dd036bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O celular é bom.', 'A bateria dura pouco.']\n",
      "Quantidade: 2\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "texto_longo = \"O celular é bom. A bateria dura pouco.\"\n",
    "\n",
    "# Tokenização por sentenças\n",
    "tokens_sentenca = sent_tokenize(texto_longo, language='portuguese')\n",
    "\n",
    "print(tokens_sentenca)\n",
    "print(f\"Quantidade: {len(tokens_sentenca)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee34abd-12a4-45dd-b3fa-793b82fcab31",
   "metadata": {},
   "source": [
    "word_tokenize (Tokenização por Palavras):\n",
    "\n",
    "    O que faz: Quebra o texto nas menores unidades de significado.\n",
    "\n",
    "    Resultado: Palavras individuais e pontuações.\n",
    "\n",
    "    Quando usar: Quando você precisa analisar o vocabulário, contar frequência de palavras ou analisar a gramática (saber o que é verbo, substantivo, etc.).\n",
    "\n",
    "    Analogia: É como desmontar a parede de Lego tijolo por tijolo.\n",
    "\n",
    "sent_tokenize (Tokenização por Sentenças):\n",
    "\n",
    "    O que faz: Quebra o texto em ideias completas (frases). Ele procura por \"pistas\" de fim de frase, como ponto final, exclamação ou interrogação.\n",
    "\n",
    "    Resultado: Frases inteiras.\n",
    "\n",
    "    Quando usar: Quando você quer analisar o sentimento de uma frase inteira ou resumir um texto parágrafo por parágrafo.\n",
    "\n",
    "    Analogia: É como separar a parede de Lego em grandes blocos ou seções, sem desmontar os tijolinhos ainda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0acc2-d273-4ff5-8acb-6c8a3d84a3f4",
   "metadata": {},
   "source": [
    "SpaCy -> Processamento e Análise Gramatical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40b357f-2019-4f0a-82a8-dc6cb9031f23",
   "metadata": {},
   "source": [
    "Baixar o modelo de língua portuguesa do SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f0853c0-f178-4a10-a6a2-4f6ef9582c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo carregado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Normalmente precisamos baixar o modelo antes no terminal com: \n",
    "# python -m spacy download pt_core_news_sm\n",
    "# Mas no Jupyter, podemos tentar carregar direto se já estiver instalado:\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"pt_core_news_sm\")\n",
    "    print(\"Modelo carregado com sucesso!\")\n",
    "except OSError:\n",
    "    print(\"Precisamos baixar o modelo. Tente rodar no terminal: python -m spacy download pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce08f6ca-2a97-4c60-8582-74a6dc1ab1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase: Péssimo atendimento, nunca mais compro.\n",
      "\n",
      "TOKEN           POS (Classe)    EXPLICAÇÃO\n",
      "---------------------------------------------\n",
      "Péssimo         ADJ             adjective\n",
      "atendimento     NOUN            noun\n",
      ",               PUNCT           punctuation\n",
      "nunca           ADV             adverb\n",
      "mais            ADV             adverb\n",
      "compro          ADJ             adjective\n",
      ".               PUNCT           punctuation\n"
     ]
    }
   ],
   "source": [
    "# Pegando a frase do índice 4 do nosso DataFrame\n",
    "texto_exemplo = df['texto_review'][3]\n",
    "\n",
    "# A mágica acontece aqui: O SpaCy processa tudo e guarda no objeto 'doc'\n",
    "doc = nlp(texto_exemplo)\n",
    "\n",
    "# Vamos ver a classificação (POS - Part of Speech) de cada token\n",
    "print(f\"Frase: {texto_exemplo}\\n\")\n",
    "print(f\"{'TOKEN':<15} {'POS (Classe)':<15} {'EXPLICAÇÃO'}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for token in doc:\n",
    "    # token.text é a palavra\n",
    "    # token.pos_ é a etiqueta gramatical (POS Tag)\n",
    "    print(f\"{token.text:<15} {token.pos_:<15} {spacy.explain(token.pos_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd1c80-a6dd-4736-ac25-50df45050e8a",
   "metadata": {},
   "source": [
    "Testes revelaram exatamente por que PLN é uma área tão desafiadora (e divertida). Vamos analisar o \"raciocínio\" da máquina nesses erros:\n",
    "\n",
    "    \"Comprei\" virou PROPN (Nome Próprio):\n",
    "\n",
    "        PROPN é a etiqueta para nomes como \"Maria\", \"Brasil\" ou \"Google\".\n",
    "\n",
    "        Por que o erro? Como a palavra \"Comprei\" estava no início da frase, ela tinha letra maiúscula. O modelo sm (small/pequeno) do SpaCy é otimizado para ser leve e rápido, então ele usa atalhos estatísticos. Ele viu a maiúscula e \"chutou\" que era um nome, em vez de analisar a conjugação verbal.\n",
    "\n",
    "    \"Compro\" virou ADJ (Adjetivo):\n",
    "\n",
    "        Por que o erro? A frase \"nunca mais compro\" tem uma estrutura de negação. O modelo se confundiu com o contexto e achou que \"compro\" estava qualificando algo, em vez de ser a ação.\n",
    "\n",
    "A Lição de Ouro aqui: Modelos de PLN são probabilísticos, não exatos. O modelo pt_core_news_sm acerta muito, mas desliza em ambiguidades. Em projetos reais profissionais, costumamos usar o modelo lg (large/grande), que é bem mais pesado (centenas de MBs), mas entende essas nuances muito melhor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52df1f59-0fe6-44d4-8527-5a9a12eef454",
   "metadata": {},
   "source": [
    "1. Os Atributos do SpaCy (.text e .pos_)\n",
    "\n",
    "Esses são atributos nativos dos objetos Token do SpaCy. Quando o nlp processa o texto, ele cria esses objetos e já preenche essas informações para você.\n",
    "\n",
    "    token.text: É a palavra original, do jeito que estava no texto (ex: \"celular\").\n",
    "\n",
    "    token.pos_: É a etiqueta da classe gramatical (ex: \"NOUN\", \"ADJ\").\n",
    "\n",
    "        Curiosidade: Notou esse sublinhado _ no final? O SpaCy faz isso porque ele guarda internamente as informações como números (para economizar memória). O _ diz: \"me mostre a versão legível em texto (string) desse código numérico\".\n",
    "\n",
    "2. A Formatação do Python (:<15)\n",
    "\n",
    "Essa parte (:<15) não é do SpaCy, é um recurso do próprio Python chamado f-string formatting. Serve apenas para deixar o visual bonito e organizado, como uma tabela.\n",
    "\n",
    "Dentro das chaves { }, os dois pontos : avisam que vamos começar uma regra de formatação.\n",
    "\n",
    "    < (Menor que): Significa alinhar à esquerda (o texto encosta na esquerda).\n",
    "\n",
    "    15: Significa reservar 15 espaços para esse texto.\n",
    "\n",
    "Visualmente, acontece isso: Imagine que a palavra é \"bom\" (3 letras).\n",
    "\n",
    "    Sem formatação: \"bom\"\n",
    "\n",
    "    Com :<15: \"bom \" (ele preenche com espaços vazios até dar 15 caracteres).\n",
    "\n",
    "Isso garante que a próxima coluna comece sempre na mesma posição, criando aquele visual de tabela alinhada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c14d5ea1-6ec4-4b4d-a5cd-92b74903d5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        texto_review adjetivos_encontrados\n",
      "0             O produto é excelente e chegou rápido!           [excelente]\n",
      "1  Não gostei, a caixa veio amassada e o produto ...                    []\n",
      "2       Comprei para minha mãe, ela adorou as cores.                    []\n",
      "3            Péssimo atendimento, nunca mais compro.     [Péssimo, compro]\n",
      "4         O celular é bom, mas a bateria dura pouco.                 [bom]\n"
     ]
    }
   ],
   "source": [
    "# 1. Criamos uma função que recebe um texto e devolve só os adjetivos\n",
    "def extrair_adjetivos(texto):\n",
    "    doc = nlp(texto)\n",
    "    # Lista com o texto de cada token, MAS SÓ SE o POS for \"ADJ\"\n",
    "    adjetivos = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "    return adjetivos\n",
    "\n",
    "# 2. Aplicamos essa função na coluna inteira e criamos uma NOVA coluna\n",
    "# df['nome_da_nova_coluna'] = df['coluna_original'].apply(funcao)\n",
    "df['adjetivos_encontrados'] = df['texto_review'].apply(extrair_adjetivos)\n",
    "\n",
    "# Mostramos o resultado\n",
    "print(df[['texto_review', 'adjetivos_encontrados']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5035587b-008d-4593-b5ec-1a4d7989b1b7",
   "metadata": {},
   "source": [
    "Joblib -> Persistência / salvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eaa88a5b-c85a-46bf-9766-21a6f66e8307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo salvo como: dataset_processado.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Definindo o nome do arquivo\n",
    "nome_arquivo = \"dataset_processado.pkl\"\n",
    "\n",
    "# Salvando (dump) o objeto 'df' inteiro no disco\n",
    "joblib.dump(df, nome_arquivo)\n",
    "\n",
    "print(f\"Arquivo salvo como: {nome_arquivo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c38687-ea32-49b3-a76f-aad4ec23eed7",
   "metadata": {},
   "source": [
    "A última etapa do seu desenho é: Nova Coluna ──(Joblib)──> Arquivo Final\n",
    "\n",
    "Em Ciência de Dados, muitas vezes o processamento demora horas. Imagine ter que rodar todo esse nlp() de novo só porque fechou o notebook? O Joblib serve para \"congelar\" seus dados processados e salvá-los no disco, mantendo toda a estrutura (listas, dicionários, tipos de variáveis) intacta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516c46b3-e0c4-45d2-9597-549812211131",
   "metadata": {},
   "source": [
    "Joblib -> Persistência / carregar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe389c42-2a6e-4f8c-a65e-79bf0b8a64ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        texto_review adjetivos_encontrados\n",
      "0             O produto é excelente e chegou rápido!           [excelente]\n",
      "1  Não gostei, a caixa veio amassada e o produto ...                    []\n",
      "2       Comprei para minha mãe, ela adorou as cores.                    []\n",
      "3            Péssimo atendimento, nunca mais compro.     [Péssimo, compro]\n",
      "4         O celular é bom, mas a bateria dura pouco.                 [bom]\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Carregando os dados do disco de volta para uma variável\n",
    "# Note que eu dei um nome novo (df_recuperado) só para provar que é um novo objeto\n",
    "df_recuperado = joblib.load('dataset_processado.pkl')\n",
    "\n",
    "# Vamos ver se a nossa coluna de adjetivos sobreviveu?\n",
    "print(df_recuperado[['texto_review', 'adjetivos_encontrados']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a94a41c-4e77-4c43-98ca-98ba874e2755",
   "metadata": {},
   "source": [
    "Em computação, esses pares são clássicos: save/load, dump/load, push/pull. O joblib segue a tradição do Python (igual à biblioteca pickle) e usa load para ler o arquivo e reconstruir seu objeto na memória."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
